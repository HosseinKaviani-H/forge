{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ SFT Training Notebook\n",
        "\n",
        "This notebook provides an interactive interface for training Language Models using Supervised Fine-Tuning (SFT).\n",
        "\n",
        "## Features\n",
        "- âœ… Interactive configuration in separate cells\n",
        "- âœ… Support for single-node and multi-node training\n",
        "- âœ… Easy hyperparameter tuning\n",
        "- âœ… Flexible parallelism strategies\n",
        "- âœ… Checkpoint management\n",
        "\n",
        "## Quick Start\n",
        "1. Configure each section (model, training, etc.)\n",
        "2. Review the complete configuration\n",
        "3. Run training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/home/hosseinkh/forge')\n",
        "\n",
        "from apps.sft_v2 import notebook_utils as nb\n",
        "import torch\n",
        "\n",
        "print(f\"âœ… Imports successful!\")\n",
        "print(f\"ðŸ“Š PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸŽ® CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ðŸ”¢ Number of GPUs: {torch.cuda.device_count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Model Configuration\n",
        "\n",
        "Configure the model you want to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "model_config = nb.create_model_config(\n",
        "    name=\"llama3\",\n",
        "    flavor=\"8B\",\n",
        "    hf_assets_path=\"/mnt/home/hosseinkh/models/Meta-Llama-3.1-8B-Instruct\"\n",
        ")\n",
        "\n",
        "print(\"ðŸ“¦ Model Configuration:\")\n",
        "for key, value in model_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Training Configuration\n",
        "\n",
        "Set training hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "training_config = nb.create_training_config(\n",
        "    local_batch_size=1,      # Batch size per GPU\n",
        "    seq_len=2048,            # Sequence length\n",
        "    max_norm=1.0,            # Gradient clipping\n",
        "    steps=1000,              # Total training steps\n",
        "    dataset=\"c4\",            # Dataset name\n",
        "    compile=False            # Use torch.compile?\n",
        ")\n",
        "\n",
        "print(\"âš™ï¸  Training Configuration:\")\n",
        "for key, value in training_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Optimizer Configuration\n",
        "\n",
        "Configure the optimizer and learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer Configuration\n",
        "optimizer_config = nb.create_optimizer_config(\n",
        "    name=\"AdamW\",\n",
        "    lr=1e-5,                 # Learning rate\n",
        "    eps=1e-8,                # Epsilon\n",
        "    weight_decay=0.0,        # Weight decay\n",
        "    betas=(0.9, 0.999)       # Adam betas\n",
        ")\n",
        "\n",
        "# LR Scheduler Configuration\n",
        "lr_scheduler_config = nb.create_lr_scheduler_config(\n",
        "    warmup_steps=200,        # Warmup steps\n",
        "    decay_steps=None,        # Decay steps (None = no decay)\n",
        "    min_lr=0.0               # Minimum LR\n",
        ")\n",
        "\n",
        "print(\"ðŸ”§ Optimizer Configuration:\")\n",
        "for key, value in optimizer_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")\n",
        "\n",
        "print(\"\\nðŸ“ˆ LR Scheduler Configuration:\")\n",
        "for key, value in lr_scheduler_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”€ Parallelism Configuration\n",
        "\n",
        "Configure distributed training strategies.\n",
        "\n",
        "### Parallelism Options:\n",
        "- **Data Parallel (Replicate)**: Basic data parallelism\n",
        "- **Data Parallel (Shard/FSDP)**: Fully Sharded Data Parallel (-1 = use all GPUs)\n",
        "- **Tensor Parallel**: Split model across multiple GPUs\n",
        "- **Pipeline Parallel**: Split model stages across GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parallelism Configuration\n",
        "parallelism_config = nb.create_parallelism_config(\n",
        "    data_parallel_replicate_degree=1,   # DP replicate\n",
        "    data_parallel_shard_degree=-1,      # FSDP (-1 = auto, uses all GPUs)\n",
        "    tensor_parallel_degree=1,           # TP\n",
        "    pipeline_parallel_degree=1,         # PP\n",
        "    context_parallel_degree=1,          # CP\n",
        "    expert_parallel_degree=1,           # EP (for MoE)\n",
        "    disable_loss_parallel=False\n",
        ")\n",
        "\n",
        "print(\"ðŸ”€ Parallelism Configuration:\")\n",
        "for key, value in parallelism_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¾ Checkpoint Configuration\n",
        "\n",
        "Configure model checkpointing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checkpoint Configuration\n",
        "checkpoint_config = nb.create_checkpoint_config(\n",
        "    enable=True,\n",
        "    folder=\"/mnt/home/hosseinkh/models/Meta-Llama-3.1-8B-Instruct/saved_checkpoints\",\n",
        "    initial_load_path=\"/mnt/home/hosseinkh/models/Meta-Llama-3.1-8B-Instruct/\",\n",
        "    initial_load_in_hf=True,\n",
        "    last_save_in_hf=True,\n",
        "    interval=500,            # Save every N steps\n",
        "    async_mode=\"disabled\"\n",
        ")\n",
        "\n",
        "# Activation Checkpoint Configuration (for memory efficiency)\n",
        "activation_checkpoint_config = nb.create_activation_checkpoint_config(\n",
        "    mode=\"selective\",        # 'selective', 'full', or 'none'\n",
        "    selective_ac_option=\"op\" # 'op' or 'layer'\n",
        ")\n",
        "\n",
        "print(\"ðŸ’¾ Checkpoint Configuration:\")\n",
        "for key, value in checkpoint_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")\n",
        "\n",
        "print(\"\\nðŸ”„ Activation Checkpoint Configuration:\")\n",
        "for key, value in activation_checkpoint_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ–¥ï¸ Resource Configuration\n",
        "\n",
        "Configure compute resources.\n",
        "\n",
        "### Options:\n",
        "- **Single Node**: Set only `procs` (number of GPUs)\n",
        "- **Multi Node**: Set both `hosts` (number of nodes) and `procs` (GPUs per node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose ONE of the following:\n",
        "\n",
        "# Option 1: Single Node (8 GPUs)\n",
        "process_config = nb.create_process_config(\n",
        "    procs=8,\n",
        "    with_gpus=True,\n",
        "    hosts=None  # None = single node\n",
        ")\n",
        "\n",
        "# Option 2: Multi-Node (4 nodes Ã— 8 GPUs = 32 total)\n",
        "# Uncomment to use:\n",
        "# process_config = nb.create_process_config(\n",
        "#     procs=8,\n",
        "#     with_gpus=True,\n",
        "#     hosts=4\n",
        "# )\n",
        "\n",
        "print(\"ðŸ–¥ï¸  Resource Configuration:\")\n",
        "for key, value in process_config.items():\n",
        "    print(f\"  â€¢ {key}: {value}\")\n",
        "\n",
        "if \"hosts\" in process_config and process_config[\"hosts\"]:\n",
        "    total_gpus = process_config[\"hosts\"] * process_config[\"procs\"]\n",
        "    print(f\"\\nðŸ“Š Total GPUs: {total_gpus}\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š Total GPUs: {process_config['procs']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## â˜ï¸ Provisioner Configuration (Optional)\n",
        "\n",
        "**Only needed for multi-node training on SLURM clusters.**\n",
        "\n",
        "âš ï¸ Skip this cell if you're running single-node training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provisioner Configuration (OPTIONAL - for multi-node only)\n",
        "# Set to None for single-node training\n",
        "\n",
        "provisioner_config = None  # Default: no provisioner\n",
        "\n",
        "# Uncomment and configure for SLURM multi-node training:\n",
        "# provisioner_config = nb.create_provisioner_config(\n",
        "#     launcher=\"slurm\",\n",
        "#     job_name=\"sft_training\",\n",
        "#     partition=\"your_gpu_partition\",  # REQUIRED for SLURM\n",
        "#     time=\"24:00:00\",                  # REQUIRED for SLURM\n",
        "#     account=\"your_account\"            # May be required\n",
        "# )\n",
        "\n",
        "if provisioner_config:\n",
        "    print(\"â˜ï¸  Provisioner Configuration:\")\n",
        "    for key, value in provisioner_config.items():\n",
        "        print(f\"  â€¢ {key}: {value}\")\n",
        "else:\n",
        "    print(\"â˜ï¸  Provisioner: Disabled (single-node mode)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¨ Build Complete Configuration\n",
        "\n",
        "Combine all configurations into a single config object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build complete configuration\n",
        "config = nb.build_config(\n",
        "    model_config=model_config,\n",
        "    optimizer_config=optimizer_config,\n",
        "    lr_scheduler_config=lr_scheduler_config,\n",
        "    training_config=training_config,\n",
        "    parallelism_config=parallelism_config,\n",
        "    checkpoint_config=checkpoint_config,\n",
        "    activation_checkpoint_config=activation_checkpoint_config,\n",
        "    process_config=process_config,\n",
        "    provisioner_config=provisioner_config\n",
        ")\n",
        "\n",
        "print(\"âœ… Configuration built successfully!\\n\")\n",
        "\n",
        "# Display summary\n",
        "nb.summarize_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“„ View Full Configuration (YAML)\n",
        "\n",
        "See the complete configuration in YAML format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print full configuration\n",
        "nb.print_config(config, title=\"Complete Training Configuration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¾ Save Configuration (Optional)\n",
        "\n",
        "Save the configuration to a YAML file for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "\n",
        "# Save configuration\n",
        "config_path = \"/home/hosseinkh/forge/apps/sft_v2/my_training_config.yaml\"\n",
        "with open(config_path, 'w') as f:\n",
        "    OmegaConf.save(config, f)\n",
        "\n",
        "print(f\"âœ… Configuration saved to: {config_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Run Training!\n",
        "\n",
        "Start the training process with the configured settings.\n",
        "\n",
        "âš ï¸ **Note**: This will start actual training and may take a long time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "print(\"ðŸš€ Starting training...\\n\")\n",
        "\n",
        "try:\n",
        "    nb.train(config)\n",
        "    print(\"\\nâœ… Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Training failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” Advanced: Step-by-Step Execution\n",
        "\n",
        "For more control, you can run each training stage separately.\n",
        "\n",
        "âš ï¸ **Only run this section if you want manual control. Otherwise, use the cell above.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Initialize provisioner (if configured)\n",
        "import asyncio\n",
        "\n",
        "provisioner_initialized = await nb.initialize_provisioner(config)\n",
        "print(f\"Provisioner initialized: {provisioner_initialized}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Create recipe\n",
        "recipe = await nb.create_recipe(config)\n",
        "print(\"Recipe created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Setup recipe (load model, data, etc.)\n",
        "await nb.setup_recipe(recipe)\n",
        "print(\"Recipe setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Run training\n",
        "await nb.train_recipe(recipe)\n",
        "print(\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Cleanup\n",
        "await nb.cleanup_recipe(recipe)\n",
        "print(\"Cleanup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Shutdown provisioner (if initialized)\n",
        "if provisioner_initialized:\n",
        "    await nb.shutdown_provisioner(config)\n",
        "    print(\"Provisioner shutdown complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Tips & Tricks\n",
        "\n",
        "### Memory Optimization\n",
        "- Use **FSDP** (set `data_parallel_shard_degree=-1`) for large models\n",
        "- Enable **activation checkpointing** (set `mode=\"selective\"` or `\"full\"`)\n",
        "- Reduce **batch size** or **sequence length**\n",
        "\n",
        "### Speed Optimization\n",
        "- Use **tensor parallelism** for large models (set `tensor_parallel_degree > 1`)\n",
        "- Enable **compilation** (set `compile=True`)\n",
        "- Increase **batch size** if memory allows\n",
        "\n",
        "### Multi-Node Training\n",
        "- Set `hosts` in process config\n",
        "- Configure provisioner with SLURM details\n",
        "- Make sure model path is accessible on all nodes\n",
        "\n",
        "### Debugging\n",
        "- Start with fewer steps (e.g., `steps=10`)\n",
        "- Use single GPU first (`procs=1`)\n",
        "- Check logs for errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Common Configurations\n",
        "\n",
        "### Quick Test Run\n",
        "```python\n",
        "training_config = nb.create_training_config(\n",
        "    steps=10,\n",
        "    local_batch_size=1\n",
        ")\n",
        "process_config = nb.create_process_config(procs=1)\n",
        "```\n",
        "\n",
        "### Single Node, 8 GPUs, FSDP\n",
        "```python\n",
        "parallelism_config = nb.create_parallelism_config(\n",
        "    data_parallel_shard_degree=-1  # Use all 8 GPUs with FSDP\n",
        ")\n",
        "process_config = nb.create_process_config(procs=8)\n",
        "```\n",
        "\n",
        "### Multi-Node, 4Ã—8 GPUs, TP=2\n",
        "```python\n",
        "parallelism_config = nb.create_parallelism_config(\n",
        "    data_parallel_shard_degree=16,   # 32 GPUs / 2 TP = 16 FSDP\n",
        "    tensor_parallel_degree=2\n",
        ")\n",
        "process_config = nb.create_process_config(procs=8, hosts=4)\n",
        "provisioner_config = nb.create_provisioner_config(\n",
        "    launcher=\"slurm\",\n",
        "    partition=\"gpu_partition\"\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
