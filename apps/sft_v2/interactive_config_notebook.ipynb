{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SFT Training - Interactive Configuration Notebook\n",
        "\n",
        "This notebook allows you to configure and run SFT training **without any YAML files**!\n",
        "\n",
        "## Benefits\n",
        "\n",
        "âœ… No external YAML files needed  \n",
        "âœ… Interactive configuration in separate cells  \n",
        "âœ… Easy to modify and experiment  \n",
        "âœ… All configuration visible in notebook  \n",
        "âœ… Quick templates for common scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import logging\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "\n",
        "from forge.apps.sft_v2.trainer_actor import TrainerActor\n",
        "from forge.apps.sft_v2.spawn_actor import SpawnActor, run_actor\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Model Settings\n",
        "\n",
        "Define your model configuration. **Modify these values as needed!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    \"name\": \"llama3\",\n",
        "    \"flavor\": \"8B\",\n",
        "    \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "}\n",
        "\n",
        "print(\"Model Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(model_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Process Settings\n",
        "\n",
        "Define how many processes to use and whether to use GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processes_config = {\n",
        "    \"procs\": 8,        # Number of processes\n",
        "    \"with_gpus\": True  # Use GPUs\n",
        "}\n",
        "\n",
        "print(\"Process Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(processes_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configure Optimizer Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer_config = {\n",
        "    \"name\": \"AdamW\",\n",
        "    \"lr\": 1e-5,    # Learning rate\n",
        "    \"eps\": 1e-8\n",
        "}\n",
        "\n",
        "print(\"Optimizer Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(optimizer_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Configure Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_scheduler_config = {\n",
        "    \"warmup_steps\": 200  # Number of warmup steps\n",
        "}\n",
        "\n",
        "print(\"LR Scheduler Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(lr_scheduler_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Configure Training Settings\n",
        "\n",
        "**Key parameters to adjust for your experiment:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_config = {\n",
        "    \"local_batch_size\": 1,  # Batch size per GPU\n",
        "    \"seq_len\": 2048,         # Sequence length\n",
        "    \"max_norm\": 1.0,         # Gradient clipping\n",
        "    \"steps\": 1000,           # Total training steps\n",
        "    \"compile\": False,        # PyTorch compilation\n",
        "    \"dataset\": \"c4\"          # Dataset name\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(training_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Configure Parallelism Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parallelism_config = {\n",
        "    \"data_parallel_replicate_degree\": 1,\n",
        "    \"data_parallel_shard_degree\": -1,  # -1 means use all available GPUs for FSDP\n",
        "    \"tensor_parallel_degree\": 1,\n",
        "    \"pipeline_parallel_degree\": 1,\n",
        "    \"context_parallel_degree\": 1,\n",
        "    \"expert_parallel_degree\": 1,\n",
        "    \"disable_loss_parallel\": False\n",
        "}\n",
        "\n",
        "print(\"Parallelism Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(parallelism_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Configure Checkpoint Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_config = {\n",
        "    \"enable\": True,\n",
        "    \"folder\": \"/tmp/Meta-Llama-3.1-8B-Instruct/saved_checkpoints\",\n",
        "    \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "    \"initial_load_in_hf\": True,\n",
        "    \"last_save_in_hf\": True,\n",
        "    \"interval\": 500,           # Save every N steps\n",
        "    \"async_mode\": \"disabled\"\n",
        "}\n",
        "\n",
        "print(\"Checkpoint Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(checkpoint_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Configure Activation Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation_checkpoint_config = {\n",
        "    \"mode\": \"selective\",\n",
        "    \"selective_ac_option\": \"op\"\n",
        "}\n",
        "\n",
        "print(\"Activation Checkpoint Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(activation_checkpoint_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Configure Communication Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comm_config = {\n",
        "    \"trace_buf_size\": 0\n",
        "}\n",
        "\n",
        "print(\"Communication Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(comm_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Combine All Configurations\n",
        "\n",
        "Now let's merge everything into a complete configuration!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all configs\n",
        "complete_config = {\n",
        "    \"comm\": comm_config,\n",
        "    \"model\": model_config,\n",
        "    \"processes\": processes_config,\n",
        "    \"optimizer\": optimizer_config,\n",
        "    \"lr_scheduler\": lr_scheduler_config,\n",
        "    \"training\": training_config,\n",
        "    \"parallelism\": parallelism_config,\n",
        "    \"checkpoint\": checkpoint_config,\n",
        "    \"activation_checkpoint\": activation_checkpoint_config\n",
        "}\n",
        "\n",
        "# Create OmegaConf DictConfig\n",
        "cfg = OmegaConf.create(complete_config)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPLETE CONFIGURATION\")\n",
        "print(\"=\" * 80)\n",
        "print(OmegaConf.to_yaml(cfg))\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Run Training (Simple Way)\n",
        "\n",
        "The simplest way - automatic lifecycle management!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training with automatic lifecycle management\n",
        "await run_actor(TrainerActor, cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Manual Lifecycle Control\n",
        "\n",
        "For more control, manage each phase separately.\n",
        "\n",
        "### Create and Spawn the Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the spawner\n",
        "spawner = SpawnActor(TrainerActor, cfg)\n",
        "\n",
        "# Spawn the actor\n",
        "actor = await spawner.spawn()\n",
        "print(f\"âœ“ Actor spawned: {actor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup the Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup (load data, checkpoints, etc.)\n",
        "await spawner.setup()\n",
        "print(\"âœ“ Actor setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "await spawner.run()\n",
        "print(\"âœ“ Training complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup resources\n",
        "await spawner.cleanup()\n",
        "print(\"âœ“ Cleanup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Quick Configuration Templates\n",
        "\n",
        "Here are ready-to-use templates for common scenarios!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template 1: Quick Test (Single GPU, Small Steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quick_test_config = OmegaConf.create({\n",
        "    \"comm\": {\"trace_buf_size\": 0},\n",
        "    \"model\": {\n",
        "        \"name\": \"llama3\",\n",
        "        \"flavor\": \"8B\",\n",
        "        \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "    },\n",
        "    \"processes\": {\"procs\": 1, \"with_gpus\": True},\n",
        "    \"optimizer\": {\"name\": \"AdamW\", \"lr\": 1e-5, \"eps\": 1e-8},\n",
        "    \"lr_scheduler\": {\"warmup_steps\": 10},\n",
        "    \"training\": {\n",
        "        \"local_batch_size\": 1,\n",
        "        \"seq_len\": 1024,\n",
        "        \"max_norm\": 1.0,\n",
        "        \"steps\": 100,  # Just 100 steps for quick testing\n",
        "        \"compile\": False,\n",
        "        \"dataset\": \"c4\"\n",
        "    },\n",
        "    \"parallelism\": {\n",
        "        \"data_parallel_replicate_degree\": 1,\n",
        "        \"data_parallel_shard_degree\": 1,\n",
        "        \"tensor_parallel_degree\": 1,\n",
        "        \"pipeline_parallel_degree\": 1,\n",
        "        \"context_parallel_degree\": 1,\n",
        "        \"expert_parallel_degree\": 1,\n",
        "        \"disable_loss_parallel\": False\n",
        "    },\n",
        "    \"checkpoint\": {\n",
        "        \"enable\": True,\n",
        "        \"folder\": \"/tmp/quick_test_checkpoints\",\n",
        "        \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "        \"initial_load_in_hf\": True,\n",
        "        \"last_save_in_hf\": True,\n",
        "        \"interval\": 50,\n",
        "        \"async_mode\": \"disabled\"\n",
        "    },\n",
        "    \"activation_checkpoint\": {\n",
        "        \"mode\": \"selective\",\n",
        "        \"selective_ac_option\": \"op\"\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"Quick Test Configuration:\")\n",
        "print(OmegaConf.to_yaml(quick_test_config))\n",
        "\n",
        "# To use: await run_actor(TrainerActor, quick_test_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template 2: Multi-GPU Training (8 GPUs with FSDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multi_gpu_config = OmegaConf.create({\n",
        "    \"comm\": {\"trace_buf_size\": 0},\n",
        "    \"model\": {\n",
        "        \"name\": \"llama3\",\n",
        "        \"flavor\": \"8B\",\n",
        "        \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "    },\n",
        "    \"processes\": {\"procs\": 8, \"with_gpus\": True},\n",
        "    \"optimizer\": {\"name\": \"AdamW\", \"lr\": 2e-5, \"eps\": 1e-8},\n",
        "    \"lr_scheduler\": {\"warmup_steps\": 200},\n",
        "    \"training\": {\n",
        "        \"local_batch_size\": 2,\n",
        "        \"seq_len\": 2048,\n",
        "        \"max_norm\": 1.0,\n",
        "        \"steps\": 5000,\n",
        "        \"compile\": False,\n",
        "        \"dataset\": \"c4\"\n",
        "    },\n",
        "    \"parallelism\": {\n",
        "        \"data_parallel_replicate_degree\": 1,\n",
        "        \"data_parallel_shard_degree\": 8,  # FSDP across 8 GPUs\n",
        "        \"tensor_parallel_degree\": 1,\n",
        "        \"pipeline_parallel_degree\": 1,\n",
        "        \"context_parallel_degree\": 1,\n",
        "        \"expert_parallel_degree\": 1,\n",
        "        \"disable_loss_parallel\": False\n",
        "    },\n",
        "    \"checkpoint\": {\n",
        "        \"enable\": True,\n",
        "        \"folder\": \"/tmp/multi_gpu_checkpoints\",\n",
        "        \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "        \"initial_load_in_hf\": True,\n",
        "        \"last_save_in_hf\": True,\n",
        "        \"interval\": 500,\n",
        "        \"async_mode\": \"disabled\"\n",
        "    },\n",
        "    \"activation_checkpoint\": {\n",
        "        \"mode\": \"selective\",\n",
        "        \"selective_ac_option\": \"op\"\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"Multi-GPU Configuration:\")\n",
        "print(OmegaConf.to_yaml(multi_gpu_config))\n",
        "\n",
        "# To use: await run_actor(TrainerActor, multi_gpu_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template 3: Memory-Efficient Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory_efficient_config = OmegaConf.create({\n",
        "    \"comm\": {\"trace_buf_size\": 0},\n",
        "    \"model\": {\n",
        "        \"name\": \"llama3\",\n",
        "        \"flavor\": \"8B\",\n",
        "        \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "    },\n",
        "    \"processes\": {\"procs\": 4, \"with_gpus\": True},\n",
        "    \"optimizer\": {\"name\": \"AdamW\", \"lr\": 1e-5, \"eps\": 1e-8},\n",
        "    \"lr_scheduler\": {\"warmup_steps\": 150},\n",
        "    \"training\": {\n",
        "        \"local_batch_size\": 1,  # Small batch size\n",
        "        \"seq_len\": 1024,         # Shorter sequence\n",
        "        \"max_norm\": 1.0,\n",
        "        \"steps\": 2000,\n",
        "        \"compile\": False,\n",
        "        \"dataset\": \"c4\"\n",
        "    },\n",
        "    \"parallelism\": {\n",
        "        \"data_parallel_replicate_degree\": 1,\n",
        "        \"data_parallel_shard_degree\": 4,\n",
        "        \"tensor_parallel_degree\": 1,\n",
        "        \"pipeline_parallel_degree\": 1,\n",
        "        \"context_parallel_degree\": 1,\n",
        "        \"expert_parallel_degree\": 1,\n",
        "        \"disable_loss_parallel\": False\n",
        "    },\n",
        "    \"checkpoint\": {\n",
        "        \"enable\": True,\n",
        "        \"folder\": \"/tmp/memory_efficient_checkpoints\",\n",
        "        \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "        \"initial_load_in_hf\": True,\n",
        "        \"last_save_in_hf\": True,\n",
        "        \"interval\": 400,\n",
        "        \"async_mode\": \"disabled\"\n",
        "    },\n",
        "    \"activation_checkpoint\": {\n",
        "        \"mode\": \"selective\",  # Saves memory\n",
        "        \"selective_ac_option\": \"op\"\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"Memory-Efficient Configuration:\")\n",
        "print(OmegaConf.to_yaml(memory_efficient_config))\n",
        "\n",
        "# To use: await run_actor(TrainerActor, memory_efficient_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Tips & Tricks\n",
        "\n",
        "## Memory Optimization\n",
        "- â¬‡ï¸ Reduce `seq_len` if running out of memory\n",
        "- â¬‡ï¸ Reduce `local_batch_size` if running out of memory\n",
        "- âœ… Enable `activation_checkpoint` for memory savings\n",
        "\n",
        "## Training Speed\n",
        "- â¬†ï¸ Increase `local_batch_size` for faster training (if memory allows)\n",
        "- ğŸš€ Use multiple GPUs with FSDP (`data_parallel_shard_degree > 1`)\n",
        "- âš¡ Enable `compile: true` for PyTorch compilation (experimental)\n",
        "\n",
        "## Debugging\n",
        "- ğŸ§ª Start with small `steps` (e.g., 10-100) to test quickly\n",
        "- ğŸ” Use single GPU first (`procs: 1`)\n",
        "- ğŸ“Š Monitor loss values in logs\n",
        "\n",
        "## Checkpoint Management\n",
        "- ğŸ’¾ Set `interval` based on how often you want to save\n",
        "- ğŸ“ Ensure `folder` path exists and has enough space\n",
        "- ğŸ”„ Use `initial_load_path` to resume from checkpoints"
      ]
    }
  ],
  "metadata": {
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
