{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SFT Training - Interactive Configuration Notebook\n",
        "\n",
        "This notebook allows you to configure and run SFT training **without any YAML files**!\n",
        "\n",
        "## Benefits\n",
        "\n",
        "✅ No external YAML files needed  \n",
        "✅ Interactive configuration in separate cells  \n",
        "✅ Easy to modify and experiment  \n",
        "✅ All configuration visible in notebook  \n",
        "✅ Quick templates for common scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import logging\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "\n",
        "from forge.apps.sft_v2.trainer_actor import TrainerActor\n",
        "from forge.apps.sft_v2.spawn_actor import SpawnActor, run_actor\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Model Settings\n",
        "\n",
        "Define your model configuration. **Modify these values as needed!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    \"name\": \"llama3\",\n",
        "    \"flavor\": \"8B\",\n",
        "    \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "}\n",
        "\n",
        "print(\"Model Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(model_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configure Process Settings\n",
        "\n",
        "Define how many processes to use and whether to use GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processes_config = {\n",
        "    \"procs\": 8,        # Number of processes\n",
        "    \"with_gpus\": True  # Use GPUs\n",
        "}\n",
        "\n",
        "print(\"Process Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(processes_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configure Optimizer Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer_config = {\n",
        "    \"name\": \"AdamW\",\n",
        "    \"lr\": 1e-5,    # Learning rate\n",
        "    \"eps\": 1e-8\n",
        "}\n",
        "\n",
        "print(\"Optimizer Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(optimizer_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Configure Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr_scheduler_config = {\n",
        "    \"warmup_steps\": 200  # Number of warmup steps\n",
        "}\n",
        "\n",
        "print(\"LR Scheduler Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(lr_scheduler_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Configure Training Settings\n",
        "\n",
        "**Key parameters to adjust for your experiment:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_config = {\n",
        "    \"local_batch_size\": 1,  # Batch size per GPU\n",
        "    \"seq_len\": 2048,         # Sequence length\n",
        "    \"max_norm\": 1.0,         # Gradient clipping\n",
        "    \"steps\": 1000,           # Total training steps\n",
        "    \"compile\": False,        # PyTorch compilation\n",
        "    \"dataset\": \"c4\"          # Dataset name\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(training_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Configure Parallelism Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parallelism_config = {\n",
        "    \"data_parallel_replicate_degree\": 1,\n",
        "    \"data_parallel_shard_degree\": -1,  # -1 means use all available GPUs for FSDP\n",
        "    \"tensor_parallel_degree\": 1,\n",
        "    \"pipeline_parallel_degree\": 1,\n",
        "    \"context_parallel_degree\": 1,\n",
        "    \"expert_parallel_degree\": 1,\n",
        "    \"disable_loss_parallel\": False\n",
        "}\n",
        "\n",
        "print(\"Parallelism Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(parallelism_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Configure Checkpoint Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_config = {\n",
        "    \"enable\": True,\n",
        "    \"folder\": \"/tmp/Meta-Llama-3.1-8B-Instruct/saved_checkpoints\",\n",
        "    \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "    \"initial_load_in_hf\": True,\n",
        "    \"last_save_in_hf\": True,\n",
        "    \"interval\": 500,           # Save every N steps\n",
        "    \"async_mode\": \"disabled\"\n",
        "}\n",
        "\n",
        "print(\"Checkpoint Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(checkpoint_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Configure Activation Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation_checkpoint_config = {\n",
        "    \"mode\": \"selective\",\n",
        "    \"selective_ac_option\": \"op\"\n",
        "}\n",
        "\n",
        "print(\"Activation Checkpoint Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(activation_checkpoint_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Configure Communication Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comm_config = {\n",
        "    \"trace_buf_size\": 0\n",
        "}\n",
        "\n",
        "print(\"Communication Configuration:\")\n",
        "print(OmegaConf.to_yaml(OmegaConf.create(comm_config)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Combine All Configurations\n",
        "\n",
        "Now let's merge everything into a complete configuration!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all configs\n",
        "complete_config = {\n",
        "    \"comm\": comm_config,\n",
        "    \"model\": model_config,\n",
        "    \"processes\": processes_config,\n",
        "    \"optimizer\": optimizer_config,\n",
        "    \"lr_scheduler\": lr_scheduler_config,\n",
        "    \"training\": training_config,\n",
        "    \"parallelism\": parallelism_config,\n",
        "    \"checkpoint\": checkpoint_config,\n",
        "    \"activation_checkpoint\": activation_checkpoint_config\n",
        "}\n",
        "\n",
        "# Create OmegaConf DictConfig\n",
        "cfg = OmegaConf.create(complete_config)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPLETE CONFIGURATION\")\n",
        "print(\"=\" * 80)\n",
        "print(OmegaConf.to_yaml(cfg))\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Run Training (Simple Way)\n",
        "\n",
        "The simplest way - automatic lifecycle management!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training with automatic lifecycle management\n",
        "await run_actor(TrainerActor, cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Manual Lifecycle Control\n",
        "\n",
        "For more control, manage each phase separately.\n",
        "\n",
        "### Create and Spawn the Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the spawner\n",
        "spawner = SpawnActor(TrainerActor, cfg)\n",
        "\n",
        "# Spawn the actor\n",
        "actor = await spawner.spawn()\n",
        "print(f\"✓ Actor spawned: {actor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup the Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup (load data, checkpoints, etc.)\n",
        "await spawner.setup()\n",
        "print(\"✓ Actor setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "await spawner.run()\n",
        "print(\"✓ Training complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup resources\n",
        "await spawner.cleanup()\n",
        "print(\"✓ Cleanup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Quick Configuration Templates\n",
        "\n",
        "Here are ready-to-use templates for common scenarios!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template 1: Quick Test (Single GPU, Small Steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quick_test_config = OmegaConf.create({\n",
        "    \"comm\": {\"trace_buf_size\": 0},\n",
        "    \"model\": {\n",
        "        \"name\": \"llama3\",\n",
        "        \"flavor\": \"8B\",\n",
        "        \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "    },\n",
        "    \"processes\": {\"procs\": 1, \"with_gpus\": True},\n",
        "    \"optimizer\": {\"name\": \"AdamW\", \"lr\": 1e-5, \"eps\": 1e-8},\n",
        "    \"lr_scheduler\": {\"warmup_steps\": 10},\n",
        "    \"training\": {\n",
        "        \"local_batch_size\": 1,\n",
        "        \"seq_len\": 1024,\n",
        "        \"max_norm\": 1.0,\n",
        "        \"steps\": 100,  # Just 100 steps for quick testing\n",
        "        \"compile\": False,\n",
        "        \"dataset\": \"c4\"\n",
        "    },\n",
        "    \"parallelism\": {\n",
        "        \"data_parallel_replicate_degree\": 1,\n",
        "        \"data_parallel_shard_degree\": 1,\n",
        "        \"tensor_parallel_degree\": 1,\n",
        "        \"pipeline_parallel_degree\": 1,\n",
        "        \"context_parallel_degree\": 1,\n",
        "        \"expert_parallel_degree\": 1,\n",
        "        \"disable_loss_parallel\": False\n",
        "    },\n",
        "    \"checkpoint\": {\n",
        "        \"enable\": True,\n",
        "        \"folder\": \"/tmp/quick_test_checkpoints\",\n",
        "        \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "        \"initial_load_in_hf\": True,\n",
        "        \"last_save_in_hf\": True,\n",
        "        \"interval\": 50,\n",
        "        \"async_mode\": \"disabled\"\n",
        "    },\n",
        "    \"activation_checkpoint\": {\n",
        "        \"mode\": \"selective\",\n",
        "        \"selective_ac_option\": \"op\"\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"Quick Test Configuration:\")\n",
        "print(OmegaConf.to_yaml(quick_test_config))\n",
        "\n",
        "# To use: await run_actor(TrainerActor, quick_test_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template 2: Multi-GPU Training (8 GPUs with FSDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multi_gpu_config = OmegaConf.create({\n",
        "    \"comm\": {\"trace_buf_size\": 0},\n",
        "    \"model\": {\n",
        "        \"name\": \"llama3\",\n",
        "        \"flavor\": \"8B\",\n",
        "        \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "    },\n",
        "    \"processes\": {\"procs\": 8, \"with_gpus\": True},\n",
        "    \"optimizer\": {\"name\": \"AdamW\", \"lr\": 2e-5, \"eps\": 1e-8},\n",
        "    \"lr_scheduler\": {\"warmup_steps\": 200},\n",
        "    \"training\": {\n",
        "        \"local_batch_size\": 2,\n",
        "        \"seq_len\": 2048,\n",
        "        \"max_norm\": 1.0,\n",
        "        \"steps\": 5000,\n",
        "        \"compile\": False,\n",
        "        \"dataset\": \"c4\"\n",
        "    },\n",
        "    \"parallelism\": {\n",
        "        \"data_parallel_replicate_degree\": 1,\n",
        "        \"data_parallel_shard_degree\": 8,  # FSDP across 8 GPUs\n",
        "        \"tensor_parallel_degree\": 1,\n",
        "        \"pipeline_parallel_degree\": 1,\n",
        "        \"context_parallel_degree\": 1,\n",
        "        \"expert_parallel_degree\": 1,\n",
        "        \"disable_loss_parallel\": False\n",
        "    },\n",
        "    \"checkpoint\": {\n",
        "        \"enable\": True,\n",
        "        \"folder\": \"/tmp/multi_gpu_checkpoints\",\n",
        "        \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "        \"initial_load_in_hf\": True,\n",
        "        \"last_save_in_hf\": True,\n",
        "        \"interval\": 500,\n",
        "        \"async_mode\": \"disabled\"\n",
        "    },\n",
        "    \"activation_checkpoint\": {\n",
        "        \"mode\": \"selective\",\n",
        "        \"selective_ac_option\": \"op\"\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"Multi-GPU Configuration:\")\n",
        "print(OmegaConf.to_yaml(multi_gpu_config))\n",
        "\n",
        "# To use: await run_actor(TrainerActor, multi_gpu_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template 3: Memory-Efficient Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory_efficient_config = OmegaConf.create({\n",
        "    \"comm\": {\"trace_buf_size\": 0},\n",
        "    \"model\": {\n",
        "        \"name\": \"llama3\",\n",
        "        \"flavor\": \"8B\",\n",
        "        \"hf_assets_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct\"\n",
        "    },\n",
        "    \"processes\": {\"procs\": 4, \"with_gpus\": True},\n",
        "    \"optimizer\": {\"name\": \"AdamW\", \"lr\": 1e-5, \"eps\": 1e-8},\n",
        "    \"lr_scheduler\": {\"warmup_steps\": 150},\n",
        "    \"training\": {\n",
        "        \"local_batch_size\": 1,  # Small batch size\n",
        "        \"seq_len\": 1024,         # Shorter sequence\n",
        "        \"max_norm\": 1.0,\n",
        "        \"steps\": 2000,\n",
        "        \"compile\": False,\n",
        "        \"dataset\": \"c4\"\n",
        "    },\n",
        "    \"parallelism\": {\n",
        "        \"data_parallel_replicate_degree\": 1,\n",
        "        \"data_parallel_shard_degree\": 4,\n",
        "        \"tensor_parallel_degree\": 1,\n",
        "        \"pipeline_parallel_degree\": 1,\n",
        "        \"context_parallel_degree\": 1,\n",
        "        \"expert_parallel_degree\": 1,\n",
        "        \"disable_loss_parallel\": False\n",
        "    },\n",
        "    \"checkpoint\": {\n",
        "        \"enable\": True,\n",
        "        \"folder\": \"/tmp/memory_efficient_checkpoints\",\n",
        "        \"initial_load_path\": \"/tmp/Meta-Llama-3.1-8B-Instruct/\",\n",
        "        \"initial_load_in_hf\": True,\n",
        "        \"last_save_in_hf\": True,\n",
        "        \"interval\": 400,\n",
        "        \"async_mode\": \"disabled\"\n",
        "    },\n",
        "    \"activation_checkpoint\": {\n",
        "        \"mode\": \"selective\",  # Saves memory\n",
        "        \"selective_ac_option\": \"op\"\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"Memory-Efficient Configuration:\")\n",
        "print(OmegaConf.to_yaml(memory_efficient_config))\n",
        "\n",
        "# To use: await run_actor(TrainerActor, memory_efficient_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Tips & Tricks\n",
        "\n",
        "## Memory Optimization\n",
        "- ⬇️ Reduce `seq_len` if running out of memory\n",
        "- ⬇️ Reduce `local_batch_size` if running out of memory\n",
        "- ✅ Enable `activation_checkpoint` for memory savings\n",
        "\n",
        "## Training Speed\n",
        "- ⬆️ Increase `local_batch_size` for faster training (if memory allows)\n",
        "- 🚀 Use multiple GPUs with FSDP (`data_parallel_shard_degree > 1`)\n",
        "- ⚡ Enable `compile: true` for PyTorch compilation (experimental)\n",
        "\n",
        "## Debugging\n",
        "- 🧪 Start with small `steps` (e.g., 10-100) to test quickly\n",
        "- 🔍 Use single GPU first (`procs: 1`)\n",
        "- 📊 Monitor loss values in logs\n",
        "\n",
        "## Checkpoint Management\n",
        "- 💾 Set `interval` based on how often you want to save\n",
        "- 📁 Ensure `folder` path exists and has enough space\n",
        "- 🔄 Use `initial_load_path` to resume from checkpoints"
      ]
    }
  ],
  "metadata": {
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
